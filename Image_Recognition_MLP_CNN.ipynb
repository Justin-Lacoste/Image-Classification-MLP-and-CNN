{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### IMPORTS ###\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HxicNYGmgP4C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "eF8NxZpHpz3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPK5Hxw-fyyv",
        "outputId": "90102a37-5a24-4ce8-ad27-cab0d2edb0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "Train: X=(50000, 3073), y=(50000, 10)\n",
            "Test: X=(10000, 3073), y=(10000, 10)\n"
          ]
        }
      ],
      "source": [
        "### LOAD AND PREPARE DATA ###\n",
        "\n",
        "#Load data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "#Vectorize images\n",
        "x_train = np.array([np.float64(x.flatten()) for x in x_train])\n",
        "x_test = np.array([np.float64(x.flatten()) for x in x_test])\n",
        "\n",
        "\n",
        "#Normalize images\n",
        "x_train -= np.mean(x_train, axis = 0)\n",
        "x_train /= np.std(x_train, axis = 0)\n",
        "x_test -= np.mean(np.float64(x_test), axis = 0)\n",
        "x_test /= np.std(x_test, axis = 0)\n",
        "\n",
        "#One hot encoding of labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "\n",
        "#Insert '1' for bias\n",
        "x_train = np.insert(x_train, 0, [1] * len(x_train), axis=1)\n",
        "x_test = np.insert(x_test, 0, [1] * len(x_test), axis=1)\n",
        "\n",
        "\n",
        "print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\n",
        "print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiLayer Perceptron Implementation"
      ],
      "metadata": {
        "id": "GVIBrrIiqDuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerPerceptron:\n",
        "\n",
        "  def __init__(self, activation_function, num_hidden_layers, hidden_layers_width):\n",
        "    self.activation_function = activation_function\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.hidden_layers_width = hidden_layers_width\n",
        "    self.loss_per_epoch = []\n",
        "\n",
        "\n",
        "    #Initialize weights with given number of hidden layers (0, 1 or 2)\n",
        "    if num_hidden_layers == 0:\n",
        "      self.w1 = np.random.rand(10, 3073)/100\n",
        "\n",
        "    elif num_hidden_layers == 1:\n",
        "      if len(hidden_layers_width) != 1:\n",
        "        raise Exception(\"Invalid input: len(hidden_layers_width) != num_hidden_layers\")\n",
        "      self.w1 = np.random.rand(hidden_layers_width[0], 3073)/100\n",
        "      self.w2 = np.random.rand(10, hidden_layers_width[0])/100\n",
        "\n",
        "    elif num_hidden_layers == 2:\n",
        "      if len(hidden_layers_width) != 2:\n",
        "        raise Exception(\"Invalid input: len(hidden_layers_width) != num_hidden_layers\")\n",
        "      self.w1 = np.random.rand(hidden_layers_width[0], 3073)/100\n",
        "      self.w2 = np.random.rand(hidden_layers_width[1], hidden_layers_width[0])/100\n",
        "      self.w3 = np.random.rand(10, hidden_layers_width[1])/100\n",
        "    else:\n",
        "      raise Exception(\"Unsupported number of hidden layers\")\n",
        "\n",
        "\n",
        "  def fit(self, x, y, learning_rate, epsilon, max_iters, batch_size):\n",
        "\n",
        "    num_of_batches = int(len(x)/batch_size)\n",
        "    x_batches = np.array_split(x, num_of_batches)\n",
        "    y_batches = np.array_split(y, num_of_batches)\n",
        "\n",
        "    #Gradient descent\n",
        "    norms = np.array([np.inf])\n",
        "    t = 0\n",
        "    print(\"Epochs: \")\n",
        "    \n",
        "    #RELU WITH 0 HIDDEN LAYERS\n",
        "    if self.activation_function == self.relu and self.num_hidden_layers == 0:\n",
        "      while np.any(norms > epsilon) and t < max_iters:\n",
        "          for batch in range(num_of_batches):\n",
        "            grad = self.relu_gradient(x_batches[batch], y_batches[batch])\n",
        "            self.w1 -= learning_rate * grad #* (1/num_of_batches)\n",
        "          t += 1\n",
        "          norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "          print(t, end=' ')\n",
        "      print(\"\")\n",
        "      print(f\"{t} iterations performed\")\n",
        "      return\n",
        "\n",
        "    #RELU WITH 1 HIDDEN LAYERS\n",
        "    elif self.activation_function == self.relu and self.num_hidden_layers == 1:\n",
        "      while np.any(norms > epsilon) and t < max_iters:\n",
        "          for batch in range(num_of_batches):\n",
        "            grad_w1, grad_w2 = self.relu_gradient(x_batches[batch], y_batches[batch])\n",
        "            self.w1 -= learning_rate * grad_w1 #* (1/num_of_batches)\n",
        "            self.w2 -= learning_rate * grad_w2 #* (1/num_of_batches)\n",
        "          t += 1\n",
        "          norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "          print(t, end=' ')\n",
        "      print(\"\")\n",
        "      print(f\"{t} iterations performed\")\n",
        "      return\n",
        "\n",
        "\n",
        "\n",
        "  def relu_gradient(self, x, y):\n",
        "\n",
        "    y_hat = self.predict(x)\n",
        "    self.loss_per_epoch.append(MultiLayerPerceptron.total_loss(y, y_hat))\n",
        "\n",
        "    if self.num_hidden_layers == 0:\n",
        "      dy = y_hat - y\n",
        "      #print(f\"dy: {dy.shape} - x: {x.shape}\")\n",
        "      dw1 = np.matmul(np.transpose(dy), x)\n",
        "      #print(\"dw1 shape before: \", dw1.shape)\n",
        "      #dw1 = np.array([np.sum(dw1, axis=0)])\n",
        "      return dw1\n",
        "\n",
        "    elif self.num_hidden_layers == 1:\n",
        "      dy = np.transpose(y_hat - y)\n",
        "      dw2 = np.matmul(dy, np.transpose(self.z1_for_gradient))\n",
        "      dw1 = np.matmul(np.transpose(dy), self.w2)\n",
        "      derivative_q1 = (self.q1_for_gradient > 0).astype(int)\n",
        "      print(f\"shapes: {dw2.shape} -- {self.w2.shape}\")\n",
        "      dw1 = np.matmul(dw1, derivative_q1)\n",
        "      dw1 = np.matmul(dw1, x)\n",
        "      return dw1, dw2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "\n",
        "    if self.num_hidden_layers == 0:\n",
        "      return self.softmax(np.transpose(np.dot(self.w1, np.transpose(x))))\n",
        "\n",
        "    elif self.num_hidden_layers == 1:\n",
        "      q1 = np.matmul(self.w1, np.transpose(x))\n",
        "      z1 = self.activation_function(q1)\n",
        "      self.q1_for_gradient = q1\n",
        "      self.z1_for_gradient = z1\n",
        "      return self.softmax(np.transpose(np.matmul(self.w2, z1)))\n",
        "    else:\n",
        "      z1 = self.activation_function(np.matmul(self.w1, np.transpose(x)))\n",
        "      z2 = self.activation_function(np.matmul(self.w2, z1))\n",
        "      return self.softmax(np.transpose(np.matmul(self.w3, z2)))\n",
        "\n",
        "  \n",
        "  @staticmethod\n",
        "  def relu(x):\n",
        "    R, C = x.shape\n",
        "    for i in range(R):\n",
        "      for j in range(C):\n",
        "        x[i][j] = max(x[i][j], 0)\n",
        "    return x\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    R, C = x.shape\n",
        "    for i in range(R):\n",
        "      denominator = sum([np.exp(j) for j in x[i]])\n",
        "      denominator = denominator if denominator > 0 else 1\n",
        "      for j in range(C):\n",
        "        x[i][j] = np.exp(x[i][j])/denominator\n",
        "    return x\n",
        "\n",
        "  @staticmethod\n",
        "  def accuracy(y, y_hat):\n",
        "\n",
        "    accurate_classifications = 0\n",
        "\n",
        "    for i, y in enumerate(y):\n",
        "      category = np.argmax(y)\n",
        "      predicted_category = np.argmax(y_hat[i])\n",
        "\n",
        "      if category == predicted_category:\n",
        "        accurate_classifications += 1\n",
        "\n",
        "    return accurate_classifications/len(y_hat)\n",
        "\n",
        "  @staticmethod\n",
        "  def total_loss(y, y_hat):\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i, y in enumerate(y):\n",
        "      category = np.argmax(y)\n",
        "      predicted_value = y_hat[i][category]\n",
        "      loss += (-1 * np.log(predicted_value))\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "rZxUINP8qC8O"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print((a > 2).astype(int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j11sPp4Y-z-Z",
        "outputId": "ddfeb9ef-fce3-4262-8170-318acef13423"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1]\n",
            " [1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MultiLayerPerceptron(MultiLayerPerceptron.relu, 1, [256])\n",
        "mlp.fit(x_train, y_train, learning_rate=0.000001, epsilon=0.0000001, max_iters=3, batch_size=2048)\n",
        "preds = mlp.predict(x_train)\n",
        "print(\"Accuracy: \", MultiLayerPerceptron.accuracy(y_train, preds)*100, \"%\")\n",
        "plt.plot(range(len(mlp.loss_per_epoch)), mlp.loss_per_epoch)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "_1yaz8_PzH79",
        "outputId": "9086112e-35b0-45ef-a7ac-fbc1cab3f203"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: \n",
            "shapes: (10, 256) -- (10, 256)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f2b1fe092115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLayerPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiLayerPerceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.000001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0000001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiLayerPerceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-4c6724ac8db3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, learning_rate, epsilon, max_iters, batch_size)\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mgrad_w1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_w2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_w1\u001b[0m \u001b[0;31m#* (1/num_of_batches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_w2\u001b[0m \u001b[0;31m#* (1/num_of_batches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m           \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (256,3073) (2084,3073) (256,3073) "
          ]
        }
      ]
    }
  ]
}