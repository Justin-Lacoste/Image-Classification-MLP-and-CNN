{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "HxicNYGmgP4C"
      },
      "outputs": [],
      "source": [
        "### IMPORTS ###\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF8NxZpHpz3I"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPK5Hxw-fyyv",
        "outputId": "481b01bf-a49e-4ae6-ec09-232672c09a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: X=(50000, 3073), y=(50000, 10)\n",
            "Test: X=(10000, 3073), y=(10000, 10)\n"
          ]
        }
      ],
      "source": [
        "### LOAD AND PREPARE DATA ###\n",
        "\n",
        "#Load data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "#Vectorize images\n",
        "x_train = np.array([np.float64(x.flatten()) for x in x_train])\n",
        "x_test = np.array([np.float64(x.flatten()) for x in x_test])\n",
        "\n",
        "\n",
        "#Normalize images\n",
        "x_train -= np.mean(x_train, axis = 0)\n",
        "x_train /= np.std(x_train, axis = 0)\n",
        "x_test -= np.mean(np.float64(x_test), axis = 0)\n",
        "x_test /= np.std(x_test, axis = 0)\n",
        "\n",
        "#One hot encoding of labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "\n",
        "#Insert '1' for bias\n",
        "x_train = np.insert(x_train, 0, [1] * len(x_train), axis=1)\n",
        "x_test = np.insert(x_test, 0, [1] * len(x_test), axis=1)\n",
        "\n",
        "\n",
        "print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\n",
        "print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVIBrrIiqDuN"
      },
      "source": [
        "# MultiLayer Perceptron Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "rZxUINP8qC8O"
      },
      "outputs": [],
      "source": [
        "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
        "\n",
        "class MultiLayerPerceptron:\n",
        "\n",
        "  def __init__(self, activation_function, num_hidden_layers, hidden_layers_width):\n",
        "    self.activation_function = activation_function\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.hidden_layers_width = hidden_layers_width\n",
        "    self.loss_per_epoch = []\n",
        "\n",
        "\n",
        "    #Initialize weights with given number of hidden layers (0, 1 or 2)\n",
        "    if num_hidden_layers == 0:\n",
        "      self.w1 = np.random.rand(10, 3073)/100\n",
        "\n",
        "    elif num_hidden_layers == 1:\n",
        "      if len(hidden_layers_width) != 1:\n",
        "        raise Exception(\"Invalid input: len(hidden_layers_width) != num_hidden_layers\")\n",
        "      self.w1 = np.random.rand(hidden_layers_width[0], 3073)/100\n",
        "      self.w2 = np.random.rand(10, hidden_layers_width[0])/100\n",
        "\n",
        "    elif num_hidden_layers == 2:\n",
        "      if len(hidden_layers_width) != 2:\n",
        "        raise Exception(\"Invalid input: len(hidden_layers_width) != num_hidden_layers\")\n",
        "      self.w1 = np.random.rand(hidden_layers_width[0], 3073)/100\n",
        "      self.w2 = np.random.rand(hidden_layers_width[1], hidden_layers_width[0])/100\n",
        "      self.w3 = np.random.rand(10, hidden_layers_width[1])/100\n",
        "    else:\n",
        "      raise Exception(\"Unsupported number of hidden layers\")\n",
        "\n",
        "\n",
        "  def fit(self, x, y, learning_rate, epsilon, max_iters, batch_size):\n",
        "\n",
        "    num_of_batches = int(len(x)/batch_size)\n",
        "    x_batches = np.array_split(x, num_of_batches)\n",
        "    y_batches = np.array_split(y, num_of_batches)\n",
        "\n",
        "    #Gradient descent\n",
        "    norms = np.array([np.inf])\n",
        "    t = 0\n",
        "    print(\"Epochs: \")\n",
        "    \n",
        "    #RELU WITH 0 HIDDEN LAYERS\n",
        "    if self.activation_function == self.relu and self.num_hidden_layers == 0:\n",
        "      while np.any(norms > epsilon) and t < max_iters:\n",
        "          for batch in range(num_of_batches):\n",
        "            grad = self.relu_gradient(x_batches[batch], y_batches[batch])\n",
        "            self.w1 -= learning_rate * grad #* (1/num_of_batches)\n",
        "          t += 1\n",
        "          norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "          print(t, end=' ')\n",
        "      print(\"\")\n",
        "      print(f\"{t} iterations performed\")\n",
        "      return\n",
        "\n",
        "    #RELU WITH 1 HIDDEN LAYERS\n",
        "    elif self.activation_function == self.relu and self.num_hidden_layers == 1:\n",
        "      while t < max_iters:\n",
        "          for batch in range(num_of_batches):\n",
        "            grad_w1, grad_w2 = self.relu_gradient(x_batches[batch], y_batches[batch])\n",
        "            self.w1 -= learning_rate * grad_w1 #* (1/num_of_batches)\n",
        "            self.w2 -= learning_rate * grad_w2 #* (1/num_of_batches)\n",
        "          t += 1\n",
        "          print(t, end=' ')\n",
        "      print(\"\")\n",
        "      print(f\"{t} iterations performed\")\n",
        "      return\n",
        "\n",
        "    \n",
        "    #TAN_H WITH 0 HIDDEN LAYERS\n",
        "    if self.activation_function == self.tanh and self.num_hidden_layers == 0:\n",
        "      while np.any(norms > epsilon) and t < max_iters:\n",
        "          for batch in range(num_of_batches):\n",
        "            grad = self.tanh_gradient(x_batches[batch], y_batches[batch])\n",
        "            self.w1 -= learning_rate * grad * (1/num_of_batches)\n",
        "          t += 1\n",
        "          norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "          print(t, end=' ')\n",
        "      print(\"\")\n",
        "      print(f\"{t} iterations performed\")\n",
        "      return\n",
        "\n",
        "    if self.activation_function == self.tanh and self.num_hidden_layers == 1:\n",
        "      while np.any(norms > epsilon) and t < max_iters:\n",
        "          for batch in range(num_of_batches):\n",
        "            grad_w1, grad_w2 = self.tanh_gradient(x_batches[batch], y_batches[batch])\n",
        "            self.w1 -= learning_rate * grad_w1 # * (1/num_of_batches)\n",
        "            self.w2 -= learning_rate * grad_w2 # * (1/num_of_batches)\n",
        "          t += 1\n",
        "          print(t, end=' ')\n",
        "      print(\"\")\n",
        "      print(f\"{t} iterations performed\")\n",
        "      return\n",
        "\n",
        "\n",
        "\n",
        "  def relu_gradient(self, x, y):\n",
        "\n",
        "    y_hat = self.predict(x)\n",
        "    self.loss_per_epoch.append(MultiLayerPerceptron.total_loss(y, y_hat))\n",
        "    N, D = x.shape\n",
        "\n",
        "    if self.num_hidden_layers == 0:\n",
        "      dy = y_hat - y\n",
        "      dw1 = np.matmul(np.transpose(dy), x)\n",
        "      return dw1\n",
        "\n",
        "    elif self.num_hidden_layers == 1:\n",
        "      dy = y_hat - y\n",
        "      dw2 = np.dot(np.transpose(dy), np.transpose(self.z1_for_gradient)) / N\n",
        "      derivative_q1 = (self.q1_for_gradient > 0).astype(int)\n",
        "      tmp_dw1 = np.matmul(y, self.w2)\n",
        "      tmp_dw1 = tmp_dw1 * np.transpose(derivative_q1)\n",
        "      dw1 = np.matmul(np.transpose(x), tmp_dw1)\n",
        "      dw1 = np.transpose(dw1)\n",
        "      dz = np.dot(dy, self.w2)\n",
        "      dv = np.dot(x.T, dz * derivative_q1.T).T / N\n",
        "      return dv, dw2\n",
        "\n",
        "  def tanh_gradient(self, x, y):\n",
        "    y_hat = self.predict(x)\n",
        "    self.loss_per_epoch.append(MultiLayerPerceptron.total_loss(y, y_hat))\n",
        "    N, D = x.shape\n",
        "\n",
        "    if self.num_hidden_layers == 0:\n",
        "      dy = y_hat - y\n",
        "      dw1 = np.matmul(np.transpose(dy), x)\n",
        "      return dw1\n",
        "\n",
        "    if self.num_hidden_layers == 1:\n",
        "      dy = y_hat - y\n",
        "      dw = (np.dot(self.z1_for_gradient, dy) / N).T\n",
        "      dz = np.dot(dy, self.w2)\n",
        "      dv = (np.dot(x.T, dz * (1 - (self.z1_for_gradient.T)**2)) / N).T\n",
        "      return dv, dw\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "\n",
        "    if self.num_hidden_layers == 0:\n",
        "      return self.softmax(np.transpose(np.dot(self.w1, np.transpose(x))))\n",
        "\n",
        "    elif self.num_hidden_layers == 1:\n",
        "      q1 = np.dot(self.w1, np.transpose(x))\n",
        "      z1 = self.activation_function(q1)\n",
        "      self.q1_for_gradient = q1\n",
        "      self.z1_for_gradient = z1\n",
        "      return self.softmax(np.transpose(np.dot(self.w2, z1)))\n",
        "    else:\n",
        "      z1 = self.activation_function(np.matmul(self.w1, np.transpose(x)))\n",
        "      z2 = self.activation_function(np.matmul(self.w2, z1))\n",
        "      return self.softmax(np.transpose(np.matmul(self.w3, z2)))\n",
        "\n",
        "  \n",
        "  @staticmethod\n",
        "  def relu(x):\n",
        "    return (x + np.abs(x))/2\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    R, C = x.shape\n",
        "    for i in range(R):\n",
        "      denominator = sum([np.exp(j) for j in x[i]])\n",
        "      denominator = denominator if denominator > 0 else 1\n",
        "      for j in range(C):\n",
        "        x[i][j] = np.exp(x[i][j])/denominator\n",
        "    return x\n",
        "\n",
        "  @staticmethod\n",
        "  def accuracy(y, y_hat):\n",
        "\n",
        "    accurate_classifications = 0\n",
        "\n",
        "    for i, y in enumerate(y):\n",
        "      category = np.argmax(y)\n",
        "      predicted_category = np.argmax(y_hat[i])\n",
        "\n",
        "      if category == predicted_category:\n",
        "        accurate_classifications += 1\n",
        "\n",
        "    return accurate_classifications/len(y_hat)\n",
        "\n",
        "  @staticmethod\n",
        "  def total_loss(y, y_hat):\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i, y in enumerate(y):\n",
        "      category = np.argmax(y)\n",
        "      predicted_value = y_hat[i][category]\n",
        "      loss += (-1 * np.log(predicted_value))\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "_1yaz8_PzH79",
        "outputId": "ba0aa47b-5372-436b-f5c0-962473eec850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: \n",
            "1 2 3 4 5 6 7 8 9 10 \n",
            "10 iterations performed\n",
            "Accuracy:  62.2 %\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "shuffle = list(zip(x_train, y_train))\n",
        "random.shuffle(shuffle)\n",
        "x_train, y_train = zip(*shuffle)\n",
        "num_train = 500\n",
        "\n",
        "mlp = MultiLayerPerceptron(MultiLayerPerceptron.tanh, 0, [300])\n",
        "mlp.fit(x_train[:num_train], y_train[:num_train], learning_rate=0.0004, epsilon=0.0000001, max_iters=10, batch_size=100)\n",
        "preds = mlp.predict(x_train[:num_train])\n",
        "print(\"Accuracy: \", MultiLayerPerceptron.accuracy(y_train[:num_train], preds)*100, \"%\")\n",
        "plt.plot(range(len(mlp.loss_per_epoch)), mlp.loss_per_epoch)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eF8NxZpHpz3I"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
